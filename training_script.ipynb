{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef04032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m269.6/269.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for muon-optimizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# @title 1. Install Dependencies\n",
    "!pip install -q torch transformers datasets accelerate huggingface_hub python-dotenv matplotlib liger-kernel \n",
    "!pip install -q git+https://github.com/KellerJordan/Muon # Install Muon Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40034f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Muon Optimizer enabled.\n",
      "‚úÖ Liger Kernel enabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import Muon and Liger, fallback if missing\n",
    "try:\n",
    "    from muon import MuonWithAuxAdam\n",
    "    HAS_MUON = True\n",
    "    print(\"‚úÖ Muon Optimizer enabled.\")\n",
    "except ImportError:\n",
    "    HAS_MUON = False\n",
    "    print(\"‚ö†Ô∏è Muon not found, using AdamW.\")\n",
    "\n",
    "try:\n",
    "    from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss\n",
    "    HAS_LIGER = True\n",
    "    print(\"‚úÖ Liger Kernel enabled.\")\n",
    "except ImportError:\n",
    "    HAS_LIGER = False\n",
    "    print(\"‚ö†Ô∏è Liger not found, using standard CrossEntropy.\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    # --- Architecture ---\n",
    "    dim: int = 384              \n",
    "    n_layers: int = 6\n",
    "    n_heads: int = 6               \n",
    "    n_kv_heads: int = 2\n",
    "    \n",
    "    # --- MoE Config ---\n",
    "    num_experts: int = 8            \n",
    "    num_shared_experts: int = 1\n",
    "    top_k: int = 2\n",
    "    expert_hidden_dim: int = 1024\n",
    "    \n",
    "    # --- MLA Config ---\n",
    "    kv_lora_rank: int = 64         \n",
    "    q_lora_rank: int = 256   \n",
    "    rope_theta: float = 10000.0\n",
    "    norm_eps: float = 1e-6\n",
    "    \n",
    "    # --- Training ---\n",
    "    vocab_size: int = 50257         # GPT-2 Vocab for TinyStories\n",
    "    max_seq_len: int = 512          # Short context\n",
    "    batch_size: int = 16       # Fits 14GB VRAM\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # --- Optimization ---\n",
    "    lr_decay_iters: int = 50000     \n",
    "    warmup_iters: int = 1000\n",
    "    max_lr: float = 6e-4            \n",
    "    min_lr: float = 6e-5\n",
    "    weight_decay_optim: float = 0.1\n",
    "    clip: float = 1.0\n",
    "    total_iters: int = 50000\n",
    "    eval_iters: int = 200\n",
    "    save_checkpoint_iter: int = 1000\n",
    "    dropout: float = 0.0\n",
    "    aux_loss_coef: float = 0.01\n",
    "    \n",
    "    # --- System ---\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    use_liger: bool = False # To debug negative loss\n",
    "    dataset: str = \"roneneldan/TinyStories\" \n",
    "    checkpoint_dir: str = \"checkpoints_tinystories\"\n",
    "    #hf_token: str = os.getenv(\"\")\n",
    "    hf_token: str = \"\"\n",
    "    hf_repo_id: str = \"FusionCorp/DeepSeek-V3-TinyStories\"\n",
    "    gradient_checkpointing: bool = False\n",
    "\n",
    "    # Periodic Validation\n",
    "    val_interval: int = 500         # Run validation every 500 steps\n",
    "    val_batches: int = 20           # Number of batches to check during validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da25701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. MODEL DEFINITION\n",
    "# ==========================================\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = torch.mean(x ** 2, dim=-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(var + self.eps)\n",
    "        return x * self.weight\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freq_cis):\n",
    "    xq_out = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_out = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freq_cis = freq_cis[:xq.shape[1]]\n",
    "\n",
    "    freq_cis = freq_cis.view(1, xq.shape[1], 1, -1)\n",
    "    \n",
    "    xq_out = torch.view_as_real(xq_out * freq_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_out * freq_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class MLA(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.kv_lora_rank = args.kv_lora_rank\n",
    "        \n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.w_kv_down = nn.Linear(args.dim, args.kv_lora_rank, bias=False)\n",
    "        self.w_kv_up = nn.Linear(args.kv_lora_rank, 2 * (args.n_heads * self.head_dim), bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "        \n",
    "        self.q_norm = RMSNorm(self.head_dim)\n",
    "        self.k_norm = RMSNorm(self.head_dim)\n",
    "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len))\n",
    "\n",
    "    def precompute_freqs_cis(self, dim: int, end: int, theta: float = 10000.0):\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        t = torch.arange(end, device=freqs.device)\n",
    "        freqs = torch.outer(t, freqs).float()\n",
    "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "        return freqs_cis\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xq = self.wq(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        latent_kv = self.w_kv_down(x)\n",
    "        kv = self.w_kv_up(latent_kv).view(B, T, 2, self.n_heads, self.head_dim)\n",
    "        xk, xv = kv.unbind(2)\n",
    "        \n",
    "        xq, xk = self.q_norm(xq), self.k_norm(xk)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, self.freqs_cis)\n",
    "        \n",
    "        out = F.scaled_dot_product_attention(\n",
    "            xq.transpose(1, 2), xk.transpose(1, 2), xv.transpose(1, 2), is_causal=True\n",
    "        )\n",
    "        return self.wo(out.transpose(1, 2).contiguous().view(B, T, C))\n",
    "\n",
    "class DeepSeekMoE(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.num_experts = args.num_experts\n",
    "        self.top_k = args.top_k\n",
    "        self.num_shared = args.num_shared_experts\n",
    "        self.gate = nn.Linear(args.dim, args.num_experts, bias=False)\n",
    "        \n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(args.dim, args.expert_hidden_dim, bias=False), \n",
    "                nn.SiLU(), \n",
    "                nn.Linear(args.expert_hidden_dim, args.dim, bias=False)\n",
    "            ) for _ in range(self.num_shared)\n",
    "        ])\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "             nn.Sequential(\n",
    "                nn.Linear(args.dim, args.expert_hidden_dim, bias=False), \n",
    "                nn.SiLU(), \n",
    "                nn.Linear(args.expert_hidden_dim, args.dim, bias=False)\n",
    "            ) for _ in range(self.num_experts)\n",
    "        ])\n",
    "\n",
    "        for expert in self.shared_experts:\n",
    "            expert[2].res_scale = True\n",
    "        \n",
    "        for expert in self.routed_experts:\n",
    "            expert[2].res_scale = True\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_shape = x.shape\n",
    "        x_flat = x.view(-1, original_shape[-1])\n",
    "        \n",
    "        # 1. Compute Shared Experts (always active)\n",
    "        shared_out = sum(expert(x_flat) for expert in self.shared_experts)\n",
    "        \n",
    "        # 2. Gating and Top-K\n",
    "        logits = self.gate(x_flat)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_k_weights, top_k_indices = torch.topk(probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Normalize weights so they sum to 1\n",
    "        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Aux Loss for load balancing\n",
    "        aux_loss = (probs.mean(dim=0) * logits.mean(dim=0)).sum() * self.num_experts\n",
    "        \n",
    "        final_out = torch.zeros_like(x_flat)\n",
    "        \n",
    "        # 3. Process each Expert\n",
    "        for i in range(self.num_experts):\n",
    "            # Check which tokens and which k-slots (0 or 1) use expert i\n",
    "            # mask shape: [num_tokens, top_k]\n",
    "            mask = (top_k_indices == i) #sets mask to true whenever we reach the wanted expert\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            \n",
    "            # We process this expert once for all tokens that need it.\n",
    "            # To handle multiple slots (if a token picks the same expert twice) we iterate through the k-slots.\n",
    "            for k in range(self.top_k):\n",
    "                k_mask = (top_k_indices[:, k] == i)\n",
    "                if k_mask.any():\n",
    "                    # Run expert on the relevant tokens and multiply by routing weight\n",
    "                    expert_input = x_flat[k_mask]\n",
    "                    expert_output = self.routed_experts[i](expert_input)\n",
    "                    \n",
    "                    # Add to final output\n",
    "                    final_out[k_mask] += expert_output * top_k_weights[k_mask, k].unsqueeze(-1)\n",
    "\n",
    "        return (shared_out + final_out).view(*original_shape), aux_loss\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(args.dim)\n",
    "        self.attn = MLA(args)\n",
    "        self.ffn_norm = RMSNorm(args.dim)\n",
    "        self.moe = DeepSeekMoE(args)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x + self.dropout(self.attn(self.attn_norm(x)))\n",
    "        moe_out, aux_loss = self.moe(self.ffn_norm(h))\n",
    "        return h + self.dropout(moe_out), aux_loss\n",
    "\n",
    "class DeepSeekV3(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.dim)\n",
    "        self.layers = nn.ModuleList([Block(args) for _ in range(args.n_layers)])\n",
    "        self.norm = RMSNorm(args.dim)\n",
    "        self.linear_layer = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
    "        self.embedding.weight = self.linear_layer.weight # Weight tying\n",
    "        self.last_aux_loss = 0.0\n",
    "        \n",
    "        if args.use_liger and HAS_LIGER:\n",
    "            self.le_loss = LigerFusedLinearCrossEntropyLoss()\n",
    "            \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "\n",
    "        std = 0.02 \n",
    "\n",
    "        # Check for flag\n",
    "        if hasattr(module,'res_scale')  and module.res_scale:\n",
    "            # Scale down by 1/sqrt(2 * n_layers)\n",
    "            std *= (2 * self.args.n_layers) ** -0.5\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        #total_aux_loss = 0.0\n",
    "\n",
    "        total_aux_loss = torch.tensor(0.0, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if self.training and self.args.gradient_checkpointing:\n",
    "                x, aux_loss = torch.utils.checkpoint.checkpoint(layer, x, use_reentrant=True)\n",
    "            else:\n",
    "                x, aux_loss = layer(x)\n",
    "\n",
    "            # trying x = x + 1 instead                \n",
    "            total_aux_loss = total_aux_loss + aux_loss\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        self.last_aux_loss = total_aux_loss\n",
    "        if self.args.use_liger and self.training: return x\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA PIPELINE\n",
    "# ==========================================\n",
    "def initialize_tokenizer(hf_token=None):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", token=hf_token)\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "class TinyStoriesStreamDataset(IterableDataset):\n",
    "    def __init__(self, split, tokenizer, seq_len, dataset_name, hf_token=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.dataset = load_dataset(dataset_name, split=\"train\" if split == \"train\" else \"validation\", streaming=True, token=hf_token)\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        buffer = []\n",
    "        while True:\n",
    "            try:\n",
    "                text = next(iterator)['text']\n",
    "                tokens = self.tokenizer.encode(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length = 1e6,\n",
    "                truncation = False ) # to get rid of the HF warning\n",
    "\n",
    "                tokens.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "                buffer.extend(tokens)\n",
    "                \n",
    "                while len(buffer) >= self.seq_len + 1:\n",
    "                    chunk = buffer[:self.seq_len + 1]\n",
    "                    buffer = buffer[self.seq_len + 1:]\n",
    "                    yield {'input_ids': torch.tensor(chunk[:-1]), 'labels': torch.tensor(chunk[1:])}\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.dataset) # Infinite loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f718c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. TRAINING UTILS\n",
    "# ==========================================\n",
    "def get_lr(it, args):\n",
    "    if it < args.warmup_iters: return args.max_lr * (it + 1) / args.warmup_iters\n",
    "    if it > args.lr_decay_iters: return args.min_lr\n",
    "    decay_ratio = (it - args.warmup_iters) / (args.lr_decay_iters - args.warmup_iters)\n",
    "    return args.min_lr + 0.5 * (args.max_lr - args.min_lr) * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir, exist_ok=True); return None, 0\n",
    "    files = glob.glob(os.path.join(checkpoint_dir, \"checkpoint_*.pt\"))\n",
    "    if not files: return None, 0\n",
    "    latest = max(files, key=lambda f: int(re.search(r'checkpoint_(\\d+).pt', f).group(1)))\n",
    "    return latest, int(re.search(r'checkpoint_(\\d+).pt', latest).group(1))\n",
    "\n",
    "\n",
    "def log_metrics(path, step, loss, lr):\n",
    "    file_exists = os.path.exists(path)\n",
    "    with open(path, \"a\") as f:\n",
    "        if not file_exists:\n",
    "            f.write(\"step,loss,lr\\n\")\n",
    "        f.write(f\"{step},{loss},{lr}\\n\")\n",
    "\n",
    "\n",
    "def log_generated_story(path, step, story):\n",
    "    # Check if file exists to determine if we need a main header\n",
    "    file_exists = os.path.exists(path)\n",
    "    \n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        # If new file, add a title\n",
    "        if not file_exists:\n",
    "            f.write(\"# üìñ DeepSeek-V3 TinyStories Generation Log\\n\\n\")\n",
    "            f.write(\"Tracking model progress over time.\\n\\n---\\n\\n\")\n",
    "        \n",
    "        # Write the Step and Story in Markdown format\n",
    "        f.write(f\"## Step {step}\\n\")\n",
    "        f.write(f\"```\\n{story}\\n```\\n\")\n",
    "        f.write(f\"_Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}_\\n\\n---\\n\\n\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, val_iterator, val_loader, args):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    # We check 'val_batches' to get a stable average\n",
    "    for _ in range(args.val_batches):\n",
    "        try:\n",
    "            batch = next(val_iterator)\n",
    "        except StopIteration:\n",
    "            # If we hit the end of the val stream, restart it\n",
    "            val_iterator = iter(val_loader)\n",
    "            batch = next(val_iterator)\n",
    "            \n",
    "        idx, targets = batch['input_ids'].to(args.device), batch['labels'].to(args.device)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(idx)\n",
    "            # Use standard CE loss for validation\n",
    "            loss = F.cross_entropy(out.view(-1, args.vocab_size), targets.view(-1))\n",
    "            \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    model.train() # Switch back to training mode\n",
    "    return avg_loss, val_iterator\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_story(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    # Determine device\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Encode\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    input_ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate\n",
    "    for _ in range(max_new_tokens):\n",
    "        if input_ids.size(1) > model.args.max_seq_len:\n",
    "            input_cond = input_ids[:, -model.args.max_seq_len:]\n",
    "        else:\n",
    "            input_cond = input_ids\n",
    "            \n",
    "        logits = model(input_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Greedy sampling for vibe check (stable)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        input_ids = torch.cat((input_ids, next_token), dim=1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "    decoded = tokenizer.decode(input_ids[0].tolist(), skip_special_tokens=True)\n",
    "    print(f\"\\nüìñ {decoded}\\n\")\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3bd7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Hugging Face repo: FusionCorp/DeepSeek-V3-TinyStories\n",
      "üìà Starting new metrics log.\n",
      "üìñ Starting new story log.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8012846f87824fe9816126ca7b2e30bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688660cc00034e3f984316f3f6eac0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df44555df2b40c497e063167f95a400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd486b8a2654341970e84556c240eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Device: cuda | Vocab: 50257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586f20262bfe45c88ff69a1b68257dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]WARNING:datasets.iterable_dataset:Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initialization complete. Starting training loop...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.07 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.02 GiB is free. Process 3184 has 11.71 GiB memory in use. Of the allocated memory 11.51 GiB is allocated by PyTorch, and 82.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4172039375.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚ùå HF Upload failed: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-4172039375.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_loss_coef\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_aux_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0maccum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.07 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.02 GiB is free. Process 3184 has 11.71 GiB memory in use. Of the allocated memory 11.51 GiB is allocated by PyTorch, and 82.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    # Necessary for Muon to work\n",
    "    if not dist.is_initialized():\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = '12355'\n",
    "        dist.init_process_group(backend='nccl', rank=0, world_size=1)\n",
    "\n",
    "    args = ModelArgs()\n",
    "\n",
    "    # Saving to HuggingFace\n",
    "    hf_api = HfApi(token=args.hf_token)\n",
    "\n",
    "    if args.hf_repo_id and args.hf_token:\n",
    "        try:\n",
    "            create_repo(args.hf_repo_id, repo_type=\"model\", exist_ok=True, token=args.hf_token)\n",
    "            print(f\"‚úÖ Connected to Hugging Face repo: {args.hf_repo_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not create/connect to HF repo: {e}\")\n",
    "\n",
    "    # --- FILE SETUP ---\n",
    "    csv_name = \"training_log.csv\"\n",
    "    stories_name = \"generated_samples.md\" # <--- NEW FILE\n",
    "    \n",
    "    local_csv_path = os.path.join(args.checkpoint_dir, csv_name)\n",
    "    local_stories_path = os.path.join(args.checkpoint_dir, stories_name) # <--- NEW PATH\n",
    "\n",
    "    # --- DOWNLOAD EXISTING LOGS (Resume Capability) ---\n",
    "    if args.hf_repo_id and args.hf_token:\n",
    "        # 1. Download CSV\n",
    "        try:\n",
    "            from huggingface_hub import hf_hub_download\n",
    "            hf_hub_download(repo_id=args.hf_repo_id, filename=csv_name, local_dir=args.checkpoint_dir, token=args.hf_token)\n",
    "            print(f\"üìà Downloaded existing metrics log.\")\n",
    "        except Exception:\n",
    "            print(\"üìà Starting new metrics log.\")\n",
    "\n",
    "        # 2. Download Stories Log (NEW)\n",
    "        try:\n",
    "            hf_hub_download(repo_id=args.hf_repo_id, filename=stories_name, local_dir=args.checkpoint_dir, token=args.hf_token)\n",
    "            print(f\"üìñ Downloaded existing story log.\")\n",
    "        except Exception:\n",
    "            print(\"üìñ Starting new story log.\")\n",
    "    \n",
    "    tokenizer = initialize_tokenizer(args.hf_token)\n",
    "    print(f\"üöÄ Device: {args.device} | Vocab: {len(tokenizer)}\")\n",
    "\n",
    "    model = DeepSeekV3(args).to(args.device)\n",
    "    \n",
    "    # Optimizer Groups\n",
    "    hidden_params, other_params = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            (hidden_params if p.ndim >= 2 and \"norm\" not in n and \"embedding\" not in n else other_params).append(p)\n",
    "\n",
    "    if HAS_MUON:\n",
    "        optimizer = MuonWithAuxAdam([\n",
    "            {'params': hidden_params, 'use_muon': True, 'lr': 0.02, 'weight_decay': 0.01},\n",
    "            {'params': other_params, 'use_muon': False, 'lr': args.max_lr, 'weight_decay': args.weight_decay_optim}\n",
    "        ])\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.max_lr, weight_decay=args.weight_decay_optim)\n",
    "\n",
    "    # Resume Weights\n",
    "    ckpt_path, start_step = find_latest_checkpoint(args.checkpoint_dir)\n",
    "    if ckpt_path:\n",
    "        print(f\"‚è© Resuming from {start_step}\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=args.device)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "\n",
    "    # Data\n",
    "    ds = TinyStoriesStreamDataset('train', tokenizer, args.max_seq_len, args.dataset, args.hf_token)\n",
    "    loader = DataLoader(ds, batch_size=args.batch_size, num_workers=2, pin_memory=True)\n",
    "    iterator = iter(loader)\n",
    "\n",
    "    val_ds = TinyStoriesStreamDataset('validation', tokenizer, args.max_seq_len, args.dataset, args.hf_token)\n",
    "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, num_workers=2, pin_memory=True)\n",
    "    val_iterator = iter(val_loader)\n",
    "\n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(range(start_step, args.total_iters), initial=start_step)\n",
    "    accum_loss = 0\n",
    "    \n",
    "    print(\"üöÄ Initialization complete. Starting training loop...\")\n",
    "\n",
    "    for step in range(start_step, args.total_iters):\n",
    "        # LR Schedule\n",
    "        lr = get_lr(step, args)\n",
    "        for g in optimizer.param_groups: \n",
    "            if not g.get('use_muon', False): g['lr'] = lr\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        for micro in range(args.gradient_accumulation_steps):\n",
    "            try: batch = next(iterator)\n",
    "            except: iterator = iter(loader); batch = next(iterator)\n",
    "            \n",
    "            idx, targets = batch['input_ids'].to(args.device), batch['labels'].to(args.device)\n",
    "            \n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                out = model(idx)\n",
    "                if args.use_liger and HAS_LIGER:\n",
    "                    loss = model.le_loss(model.linear_layer.weight, out.view(-1, args.dim), targets.view(-1))\n",
    "                else:\n",
    "                    loss = F.cross_entropy(out.view(-1, args.vocab_size), targets.view(-1))\n",
    "                \n",
    "                loss = (loss + args.aux_loss_coef * model.last_aux_loss) / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            accum_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- VALIDATION & SAMPLING ---\n",
    "        if step % 500 == 0 and step > start_step:\n",
    "            \n",
    "            val_loss, val_iterator = estimate_loss(model, val_iterator, val_loader, args)\n",
    "            print(f\"\\nüîç Step {step} | Train Loss: {accum_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            print(f\"‚ú® Vibe Check:\")\n",
    "            try:\n",
    "                # Generate\n",
    "                story_text = generate_story(model, tokenizer, \"Once upon a time\", max_new_tokens=100)\n",
    "                # Log to MD file\n",
    "                log_generated_story(local_stories_path, step, story_text)\n",
    "            except Exception as e:\n",
    "                print(f\"(Generation/Logging skipped: {e})\")\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step} | Loss: {accum_loss:.4f} | LR: {lr:.2e}\")\n",
    "            log_metrics(local_csv_path, step, accum_loss, lr)\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Loss: {accum_loss:.4f} | LR: {lr:.2e}\")\n",
    "        accum_loss = 0\n",
    "\n",
    "        # --- SAVING & UPLOADING ---\n",
    "        if step % args.save_checkpoint_iter == 0 and step > start_step:\n",
    "            ckpt_name = f\"checkpoint_{step}.pt\"\n",
    "            path = os.path.join(args.checkpoint_dir, ckpt_name)\n",
    "            \n",
    "            # Save Locally\n",
    "            torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'args': args}, path)\n",
    "            print(f\"\\nüíæ Saved locally to {path}\")\n",
    "\n",
    "            if args.hf_repo_id and args.hf_token:\n",
    "                try:\n",
    "                    # 1. Upload Checkpoint\n",
    "                    hf_api.upload_file(\n",
    "                        path_or_fileobj=path,\n",
    "                        path_in_repo=f\"checkpoints/{ckpt_name}\",\n",
    "                        repo_id=args.hf_repo_id\n",
    "                    )\n",
    "                    # 2. Upload CSV\n",
    "                    hf_api.upload_file(\n",
    "                        path_or_fileobj=local_csv_path,\n",
    "                        path_in_repo=csv_name,\n",
    "                        repo_id=args.hf_repo_id\n",
    "                    )\n",
    "                    # 3. Upload Stories Log (NEW)\n",
    "                    hf_api.upload_file(\n",
    "                        path_or_fileobj=local_stories_path,\n",
    "                        path_in_repo=stories_name,\n",
    "                        repo_id=args.hf_repo_id\n",
    "                    )\n",
    "                    print(f\"‚òÅÔ∏è Synced Checkpoint, CSV, and Stories to Hugging Face\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå HF Upload failed: {e}\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf68a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf63a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbab467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ac425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dee3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
