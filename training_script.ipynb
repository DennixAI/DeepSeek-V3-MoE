{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef04032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.1771 | LR: 4.40e-06:   0%|          | 11/50000 [24:59<1893:09:47, 136.34s/it]\n",
      "  0%|          | 0/50000 [29:49<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# @title 1. Install Dependencies\n",
    "!pip install -q torch transformers datasets accelerate huggingface_hub python-dotenv matplotlib liger-kernel\n",
    "!pip install -q git+https://github.com/KellerJordan/Muon # Install Muon Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40034f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Muon Optimizer enabled.\n",
      "âœ… Liger Kernel enabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import Muon and Liger, fallback if missing\n",
    "try:\n",
    "    from muon import MuonWithAuxAdam\n",
    "    HAS_MUON = True\n",
    "    print(\"âœ… Muon Optimizer enabled.\")\n",
    "except ImportError:\n",
    "    HAS_MUON = False\n",
    "    print(\"âš ï¸ Muon not found, using AdamW.\")\n",
    "\n",
    "try:\n",
    "    from liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss\n",
    "    HAS_LIGER = True\n",
    "    print(\"âœ… Liger Kernel enabled.\")\n",
    "except ImportError:\n",
    "    HAS_LIGER = False\n",
    "    print(\"âš ï¸ Liger not found, using standard CrossEntropy.\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    # --- Architecture ---\n",
    "    dim: int = 384              \n",
    "    n_layers: int = 6\n",
    "    n_heads: int = 6               \n",
    "    n_kv_heads: int = 2\n",
    "    \n",
    "    # --- MoE Config ---\n",
    "    num_experts: int = 8            \n",
    "    num_shared_experts: int = 1\n",
    "    top_k: int = 2\n",
    "    expert_hidden_dim: int = 1024\n",
    "    \n",
    "    # --- MLA Config ---\n",
    "    kv_lora_rank: int = 64         \n",
    "    q_lora_rank: int = 256   \n",
    "    rope_theta: float = 10000.0\n",
    "    norm_eps: float = 1e-6\n",
    "    \n",
    "    # --- Training ---\n",
    "    vocab_size: int = 50257         # GPT-2 Vocab for TinyStories\n",
    "    max_seq_len: int = 512          # Short context\n",
    "    batch_size: int = 32       # Fits 14GB VRAM\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    \n",
    "    # --- Optimization ---\n",
    "    lr_decay_iters: int = 50000     \n",
    "    warmup_iters: int = 1000\n",
    "    max_lr: float = 6e-4            \n",
    "    min_lr: float = 6e-5\n",
    "    weight_decay_optim: float = 0.1\n",
    "    clip: float = 1.0\n",
    "    total_iters: int = 50000\n",
    "    eval_iters: int = 200\n",
    "    save_checkpoint_iter: int = 1000\n",
    "    dropout: float = 0.0\n",
    "    aux_loss_coef: float = 0.01\n",
    "    \n",
    "    # --- System ---\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    use_liger: bool = True \n",
    "    dataset: str = \"roneneldan/TinyStories\" \n",
    "    checkpoint_dir: str = \"checkpoints_tinystories\"\n",
    "    #hf_token: str = os.getenv(\"HF_TOKEN\")\n",
    "    hf_token: str = \"\"\n",
    "    hf_repo_id: str = \"FusionCorp/DeepSeek-V3-TinyStories\"\n",
    "    gradient_checkpointing: bool = True \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da25701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. MODEL DEFINITION\n",
    "# ==========================================\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = torch.mean(x ** 2, dim=-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(var + self.eps)\n",
    "        return x * self.weight\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freq_cis):\n",
    "    xq_out = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_out = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freq_cis = freq_cis[:xq.shape[1]]\n",
    "\n",
    "    freq_cis = freq_cis.view(1, xq.shape[1], 1, -1)\n",
    "    \n",
    "    xq_out = torch.view_as_real(xq_out * freq_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_out * freq_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class MLA(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.kv_lora_rank = args.kv_lora_rank\n",
    "        \n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.w_kv_down = nn.Linear(args.dim, args.kv_lora_rank, bias=False)\n",
    "        self.w_kv_up = nn.Linear(args.kv_lora_rank, 2 * (args.n_heads * self.head_dim), bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "        \n",
    "        self.q_norm = RMSNorm(self.head_dim)\n",
    "        self.k_norm = RMSNorm(self.head_dim)\n",
    "        self.register_buffer(\"freqs_cis\", self.precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len))\n",
    "\n",
    "    def precompute_freqs_cis(self, dim: int, end: int, theta: float = 10000.0):\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        t = torch.arange(end, device=freqs.device)\n",
    "        freqs = torch.outer(t, freqs).float()\n",
    "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "        return freqs_cis\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        xq = self.wq(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        latent_kv = self.w_kv_down(x)\n",
    "        kv = self.w_kv_up(latent_kv).view(B, T, 2, self.n_heads, self.head_dim)\n",
    "        xk, xv = kv.unbind(2)\n",
    "        \n",
    "        xq, xk = self.q_norm(xq), self.k_norm(xk)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, self.freqs_cis)\n",
    "        \n",
    "        out = F.scaled_dot_product_attention(\n",
    "            xq.transpose(1, 2), xk.transpose(1, 2), xv.transpose(1, 2), is_causal=True\n",
    "        )\n",
    "        return self.wo(out.transpose(1, 2).contiguous().view(B, T, C))\n",
    "\n",
    "class DeepSeekMoE(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.num_experts = args.num_experts\n",
    "        self.top_k = args.top_k\n",
    "        self.num_shared = args.num_shared_experts\n",
    "        self.gate = nn.Linear(args.dim, args.num_experts, bias=False)\n",
    "        \n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(args.dim, args.expert_hidden_dim, bias=False), \n",
    "                nn.SiLU(), \n",
    "                nn.Linear(args.expert_hidden_dim, args.dim, bias=False)\n",
    "            ) for _ in range(self.num_shared)\n",
    "        ])\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "             nn.Sequential(\n",
    "                nn.Linear(args.dim, args.expert_hidden_dim, bias=False), \n",
    "                nn.SiLU(), \n",
    "                nn.Linear(args.expert_hidden_dim, args.dim, bias=False)\n",
    "            ) for _ in range(self.num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_shape = x.shape\n",
    "        x_flat = x.view(-1, original_shape[-1])\n",
    "        \n",
    "        # 1. Compute Shared Experts (always active)\n",
    "        shared_out = sum(expert(x_flat) for expert in self.shared_experts)\n",
    "        \n",
    "        # 2. Gating and Top-K\n",
    "        logits = self.gate(x_flat)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_k_weights, top_k_indices = torch.topk(probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Normalize weights so they sum to 1\n",
    "        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Aux Loss for load balancing\n",
    "        aux_loss = (probs.mean(dim=0) * logits.mean(dim=0)).sum() * self.num_experts\n",
    "        \n",
    "        final_out = torch.zeros_like(x_flat)\n",
    "        \n",
    "        # 3. Process each Expert\n",
    "        for i in range(self.num_experts):\n",
    "            # Check which tokens and which k-slots (0 or 1) use expert i\n",
    "            # mask shape: [num_tokens, top_k]\n",
    "            mask = (top_k_indices == i) #sets mask to true whenever we reach the wanted expert\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            \n",
    "            # We process this expert once for all tokens that need it.\n",
    "            # To handle multiple slots (if a token picks the same expert twice) we iterate through the k-slots.\n",
    "            for k in range(self.top_k):\n",
    "                k_mask = (top_k_indices[:, k] == i)\n",
    "                if k_mask.any():\n",
    "                    # Run expert on the relevant tokens and multiply by routing weight\n",
    "                    expert_input = x_flat[k_mask]\n",
    "                    expert_output = self.routed_experts[i](expert_input)\n",
    "                    \n",
    "                    # Add to final output\n",
    "                    final_out[k_mask] += expert_output * top_k_weights[k_mask, k].unsqueeze(-1)\n",
    "\n",
    "        return (shared_out + final_out).view(*original_shape), aux_loss\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(args.dim)\n",
    "        self.attn = MLA(args)\n",
    "        self.ffn_norm = RMSNorm(args.dim)\n",
    "        self.moe = DeepSeekMoE(args)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x + self.dropout(self.attn(self.attn_norm(x)))\n",
    "        moe_out, aux_loss = self.moe(self.ffn_norm(h))\n",
    "        return h + self.dropout(moe_out), aux_loss\n",
    "\n",
    "class DeepSeekV3(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.dim)\n",
    "        self.layers = nn.ModuleList([Block(args) for _ in range(args.n_layers)])\n",
    "        self.norm = RMSNorm(args.dim)\n",
    "        self.linear_layer = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
    "        self.embedding.weight = self.linear_layer.weight # Weight tying\n",
    "        self.last_aux_loss = 0.0\n",
    "        \n",
    "        if args.use_liger and HAS_LIGER:\n",
    "            self.le_loss = LigerFusedLinearCrossEntropyLoss()\n",
    "            \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        #total_aux_loss = 0.0\n",
    "\n",
    "        total_aux_loss = torch.tensor(0.0, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if self.training and self.args.gradient_checkpointing:\n",
    "                x, aux_loss = torch.utils.checkpoint.checkpoint(layer, x, use_reentrant=True)\n",
    "            else:\n",
    "                x, aux_loss = layer(x)\n",
    "\n",
    "            # trying x = x + 1 instead                \n",
    "            total_aux_loss = total_aux_loss + aux_loss\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        self.last_aux_loss = total_aux_loss\n",
    "        if self.args.use_liger and self.training: return x\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA PIPELINE\n",
    "# ==========================================\n",
    "def initialize_tokenizer(hf_token=None):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", token=hf_token)\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "class TinyStoriesStreamDataset(IterableDataset):\n",
    "    def __init__(self, split, tokenizer, seq_len, dataset_name, hf_token=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.dataset = load_dataset(dataset_name, split=\"train\" if split == \"train\" else \"validation\", streaming=True, token=hf_token)\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        buffer = []\n",
    "        while True:\n",
    "            try:\n",
    "                text = next(iterator)['text']\n",
    "                tokens = self.tokenizer.encode(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length = 1e6,\n",
    "                truncation = False ) # to get rid of the HF warning\n",
    "\n",
    "                buffer.extend(tokens)\n",
    "                while len(buffer) >= self.seq_len + 1:\n",
    "                    chunk = buffer[:self.seq_len + 1]\n",
    "                    buffer = buffer[self.seq_len + 1:]\n",
    "                    yield {'input_ids': torch.tensor(chunk[:-1]), 'labels': torch.tensor(chunk[1:])}\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.dataset) # Infinite loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f718c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [24:31<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Device: cuda | Vocab: 50257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initialization complete. Starting training loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Loss: 11.0061 | LR: 4.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | Loss: 4.3378 | LR: 4.40e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-709115747.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# Run it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-709115747.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_loss_coef\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_aux_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0maccum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. TRAINING UTILS\n",
    "# ==========================================\n",
    "def get_lr(it, args):\n",
    "    if it < args.warmup_iters: return args.max_lr * (it + 1) / args.warmup_iters\n",
    "    if it > args.lr_decay_iters: return args.min_lr\n",
    "    decay_ratio = (it - args.warmup_iters) / (args.lr_decay_iters - args.warmup_iters)\n",
    "    return args.min_lr + 0.5 * (args.max_lr - args.min_lr) * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir, exist_ok=True); return None, 0\n",
    "    files = glob.glob(os.path.join(checkpoint_dir, \"checkpoint_*.pt\"))\n",
    "    if not files: return None, 0\n",
    "    latest = max(files, key=lambda f: int(re.search(r'checkpoint_(\\d+).pt', f).group(1)))\n",
    "    return latest, int(re.search(r'checkpoint_(\\d+).pt', latest).group(1))\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN TRAINING LOOP\n",
    "# ==========================================\n",
    "def train():\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # Necessary for Muon to work\n",
    "    if not dist.is_initialized():\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = '12355'\n",
    "        dist.init_process_group(backend='nccl', rank=0, world_size=1)\n",
    "\n",
    "\n",
    "    args = ModelArgs()\n",
    "    tokenizer = initialize_tokenizer(args.hf_token)\n",
    "    print(f\"ðŸš€ Device: {args.device} | Vocab: {len(tokenizer)}\")\n",
    "\n",
    "    model = DeepSeekV3(args).to(args.device)\n",
    "    \n",
    "    # Optimizer Groups\n",
    "    hidden_params, other_params = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            (hidden_params if p.ndim >= 2 and \"norm\" not in n and \"embedding\" not in n else other_params).append(p)\n",
    "\n",
    "    if HAS_MUON:\n",
    "        optimizer = MuonWithAuxAdam([\n",
    "            {'params': hidden_params, 'use_muon': True, 'lr': 0.02, 'weight_decay': 0.01},\n",
    "            {'params': other_params, 'use_muon': False, 'lr': args.max_lr, 'weight_decay': args.weight_decay_optim}\n",
    "        ])\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.max_lr, weight_decay=args.weight_decay_optim)\n",
    "\n",
    "    # Resume\n",
    "    ckpt_path, start_step = find_latest_checkpoint(args.checkpoint_dir)\n",
    "    if ckpt_path:\n",
    "        print(f\"â© Resuming from {start_step}\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=args.device)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "\n",
    "    # Data\n",
    "    ds = TinyStoriesStreamDataset('train', tokenizer, args.max_seq_len, args.dataset, args.hf_token)\n",
    "    loader = DataLoader(ds, batch_size=args.batch_size, num_workers=2, pin_memory=True)\n",
    "    iterator = iter(loader)\n",
    "\n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(range(start_step, args.total_iters), initial=start_step)\n",
    "    accum_loss = 0\n",
    "    \n",
    "    print(\"ðŸš€ Initialization complete. Starting training loop...\")\n",
    "\n",
    "    for step in range(start_step, args.total_iters):\n",
    "        # LR Schedule\n",
    "        lr = get_lr(step, args)\n",
    "        for g in optimizer.param_groups: \n",
    "            if not g.get('use_muon', False): g['lr'] = lr\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        for micro in range(args.gradient_accumulation_steps):\n",
    "            try: batch = next(iterator)\n",
    "            except: iterator = iter(loader); batch = next(iterator)\n",
    "            \n",
    "            idx, targets = batch['input_ids'].to(args.device), batch['labels'].to(args.device)\n",
    "            \n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                out = model(idx)\n",
    "                if args.use_liger and HAS_LIGER:\n",
    "                    loss = model.le_loss(model.linear_layer.weight, out.view(-1, args.dim), targets.view(-1))\n",
    "                else:\n",
    "                    loss = F.cross_entropy(out.view(-1, args.vocab_size), targets.view(-1))\n",
    "                \n",
    "                loss = (loss + args.aux_loss_coef * model.last_aux_loss) / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            accum_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step} | Loss: {accum_loss:.4f} | LR: {lr:.2e}\")\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Loss: {accum_loss:.4f} | LR: {lr:.2e}\")\n",
    "        accum_loss = 0\n",
    "\n",
    "        if step % args.save_checkpoint_iter == 0 and step > start_step:\n",
    "            path = os.path.join(args.checkpoint_dir, f\"checkpoint_{step}.pt\")\n",
    "            torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'args': args}, path)\n",
    "            print(f\"\\nðŸ’¾ Saved {path}\")\n",
    "\n",
    "# Run it\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf68a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf63a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbab467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ac425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dee3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
